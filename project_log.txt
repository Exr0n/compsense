<entry name="first thoughts">
  It's 18 April 2019 and computers are cool. Can we get them to extrapolate information?

  Goal: Read the first line of a bunch of wikipedia pages, then parse them and somehow "remember" that information. Then, output a sentence of "common sense" that fits in with other things on conceptnet.io

  Is it doable? Probably.
  Can I do it? Probably not.

  Lets get started!
</entry>

<entry name="reality sinks in">
  So some smart people have done some cool stuff... like deepmind.

  But I want to tackle a different problem: Can we get the computer to make a "web" of information? Something like how the brain forms a "web" of knowledge?
  What I mean is: we think of things as connected to other things, and so when you think of a word many others come to mind.

  =============================================================================================================================
  + THE (main) PROBLEM: What kind of storage or "memory" system needs to be in place to allow for this kind of connectedness? +
  =============================================================================================================================

  Some kind of node like system is what comes to mind, but what does that even mean in a computing sense?
  I suppose we have to use Google's natural language API, because that will make things alot easier. It categorizes things and assigns relations to them. Maybe it even solved this problem, and I just did'nt notice.
  (I hope not, that would be pretty akward.)

  Anyways, I need to do more thinking. Cyan!
</entry>

<entry name="first tangable ideas">
  We learn and think with nodes, right?
  What if the computer stored nodes that have connections with other nodes, and those connections get stronger when the computer sees two "linked" concepts?
  This is pretty much a model at our current guess of how the brain works. Right?

  I was thinking that there could be different types of connections, like -1 < meaning < 1 connections and -1 < sentiment < 1 connections, etc,
    but that seems to defeat the whole point.

  After all, the goal is to not constrain the computer, and putting in those categories for it would make the first part easy but limit it in the future.
  It needs to learn the categories for itself. The problem is how?

  That needs to be thought out more...

  Anyways, I was thinking there are three main "layers" or "steps" to this whole thing.
    1. Analyze text (wikipedia, other sources maybe) and have some understanding of how concepts relate  (Google NatLang API already does this pretty well)
    2. Encode this basic understanding in a network of nodes, with a node for each concept.
      We want the network to be able to help with understanding new material, so it should tie back into the NatLang system if possible. But this is a next step.
    3. After the network learns a bunch of stuff, look at some of the nodes and how they are connected, to see what the network "knows".
      It would be really nice for us to be able to just ask the network what it knows about stuff, but that might be difficult.

  So that's the big picture overview. Here are the next steps:
    1. Flush out the idea of computer generated connections, and how they would work.
      This seems like the "correct" way to solve this problem, and I refuse to compromise. I don't know why, but I have this strong gut feeling that we should leave
      the connection types up to the computer.
    2. Write a preliminary class structure for the "knowledge net", or step 2. This is the meat of the project.
    3. Figure out how exactly to use the Google NatLang API and how to shove its output into the knownet.
    4. Think more about how we can get an output from the net. I wan't it to have simple statements like conceptnet, but I don't know exactly how that would work.

  To give a sense of time, I don't expect to get to step 4 until well into 2020. And I hope I don't give up before then...

  As of yet, I'm still really excited about this. More thinking is to be done. Cyan!
</entry>

<entry name="multi layer system?">
  I was doing even more thinking and I think I have an idea for how to make the relations change dynamically. The answer? More learning!
  Basically we originally had a network of concepts connected by relations. Originally, there was going to be just one type of relation, a value from -1 to 1.
  But then, I looked at conceptnet more and they have a huge variety of different relations, and those different types of relations really form the backbone of the "intuition".
  So it seems clear that we also need those different relations. The problem is that on conceptnet, those relations are generated by humans (i think?) and that's bad,
    because human input is the thing that we are trying to solve here.
  So what we have to do is make a network of relations, that are related to each other with other relations. Importantly, the other relations are not meta-relations. That would
    just cause the problem to be moved down a level. Instead, those relations are simply the same level of relations as before.
  That way, we won't have an infinite downwards spiral of crazy memory usage.

  There seem to be a few glaring problems with this system off in the distance, but I can't quite make them out through all the fog. I guess we will figure it out when it gets there!
  (Even if it means starting over (I hope not (get ready for a lot of closing parenthesis (')'))))

  So yeah, thats the concept update for now. Im gonna start figuring out the class structure and how everything is actually gonna get put together. But until then, cyan!
</entry>s
